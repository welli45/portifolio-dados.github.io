{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Otimização usando Gradient Descent em duas variáveis usando Python\"\n",
        "image: ../../Vida-Academica/images/calculo/07-gradiente-descida.png\n",
        "description: \"Anotações do módulo 2 do curso de Cálculo para aprendizado de máquina e ciência de dados\"\n",
        "author: \"Wellington Santos Souza\"\n",
        "date: \"2024-08-04\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "categories: [\"curso\", \"coursera\", \"Algebra Linear\", \"calculo\", \"Machine Learning\", \"Data Science\", \"Deep Learning\", \"Python\", \"R\", \"Algebra Linear\"]\n",
        "---\n",
        "\n",
        "\n",
        "Método de **Gradient Descent** otimizando algumas funções em duas variáveis.\n",
        "\n",
        "Vamos carregar os pacotes que utilizaremos aqui:\n"
      ],
      "id": "9c9334e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from w2_tools import (plot_f_cont_and_surf, gradient_descent_two_variables, \n",
        "                      f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                      f_example_4, dfdx_example_4, dfdy_example_4)"
      ],
      "id": "73ebe8a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos explorar um exemplo simples de uma função em duas variáveis $f(x,y)$ com um mínimo global.\n"
      ],
      "id": "dd50bfae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [74, 85], f_example_3, cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "ab64a4cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para encontrarmos o mínimo, você podemos implementar a **gradient descent** a partir do ponto inicial $(x_0,y_0)$ e realizar etapas iteração por iteração usando as seguintes equações:\n",
        "\n",
        "$$\n",
        "x_1 = x_0 - \\alpha \\frac{\\partial f}{\\partial x}(x_0, y_0),\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_1 = y_0 - \\alpha \\frac{\\partial f}{\\partial y}(x_0, y_0),\\tag{1}\n",
        "$$\n",
        "\n",
        "em que $\\alpha>0$ é uma taxa de aprendizado. O número de iterações também é um parâmetro. O método é implementado com o seguinte código:\n"
      ],
      "id": "9c048862"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(dfdx, dfdy, x, y, learning_rate = 0.1, num_iterations = 100):\n",
        "    for iteration in range(num_iterations):\n",
        "        x, y = x - learning_rate * dfdx(x, y), y - learning_rate * dfdy(x, y)\n",
        "    return x, y"
      ],
      "id": "461e8d9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para otimizar a função, configuramos os parâmetros `num_iterations`, `learning_rate`, `x_initial`, `y_initial` e executar o **gradient descent**:\n"
      ],
      "id": "5d32a068"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 30; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_3, dfdy_example_3, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "bbb57854",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode ver a visualização executando o código a seguir. Observe que a descida de gradiente em duas variáveis executa etapas no plano, em uma direção oposta ao vetor de gradiente $\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x_0, y_0) \\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0)\\end{bmatrix}$ com a taxa de aprendizado $\\alpha$ como um fator de escala.\n"
      ],
      "id": "85363c02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 20; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.5; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 3.5; y_initial = 3.6\n",
        "\n",
        "gd_example_3 = gradient_descent_two_variables([0, 5], [0, 5], [74, 85], \n",
        "                                              f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [0.1, 0.1, 81.5], 2, [4, 1, 171], \n",
        "                                              cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "e46ce791",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos dar uma olhada nessa outra função\n"
      ],
      "id": "b95a37da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [6, 9.5], f_example_4, cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "a758ed53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos encontrar o mínimo global com:\n"
      ],
      "id": "93e825f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 100; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_4, dfdy_example_4, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "66e5b39d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando\n"
      ],
      "id": "37032ce7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converges to the global minimum point.\n",
        "num_iterations = 30; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "# Converges to a local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 2; y_initial = 3\n",
        "# Converges to another local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 4; y_initial = 0.5\n",
        "\n",
        "gd_example_4 = gradient_descent_two_variables([0, 5], [0, 5], [6, 9.5], \n",
        "                                              f_example_4, dfdx_example_4, dfdy_example_4, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [2, 2, 6], 0.5, [2, 1, 63], \n",
        "                                              cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "3e9e5ab1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}